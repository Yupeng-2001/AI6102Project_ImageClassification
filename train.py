# -*- coding: utf-8 -*-
"""Copy of AI6102.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14P6VfChelYs1JyID4vQAQer5aBfQa-Ve

## data loading##

## Training of the model ##
parse parameters
"""

import argparse
import torch
import torch.nn as nn
import torchvision.models as models
import torch.optim as optim
from torch.utils.data import DataLoader
import copy
import pdb
import os
import random
import numpy as np
import datetime
import json
from data import get_dataloader, default_transform, canny_transform
from torch.utils.data import Subset, random_split
from torch.utils.data import DataLoader

class ResNetClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ResNetClassifier, self).__init__()

        # Load pre-trained ResNet50 model
        self.resnet = models.resnet50(pretrained=True)

        # Replace the final fully connected layer with a new one
        num_ftrs = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        x = self.resnet(x)
        return x

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):
  model.to(device)
  trianing_result = []
  best_model_params = copy.deepcopy(model.state_dict())
  best_valid_loss = 999999
  for epoch in range(epochs):
    # Training phase
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for images, labels in train_loader:
      images, labels = images.to(device), labels.to(device)

      #train the model
      optimizer.zero_grad()
      outputs = model(images)
      loss = criterion(outputs, labels)
      loss.backward()
      optimizer.step()

      #saving loss and acc
      running_loss += loss.item() * images.size(0)
      _, predicted = torch.max(outputs, 1)
      total += labels.size(0)
      correct += (predicted == labels).sum().item()


    #calculate loss and accuracy
    train_loss = running_loss / len(train_loader.dataset)
    train_acc = correct / total
    print(f"Epoch : {epoch}")
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)

    epoch_result = {}
    epoch_result["Epoch"] = epoch
    epoch_result["Train Loss"] = train_loss
    epoch_result["Val Loss"] = val_loss
    epoch_result["Train Acc"] = train_acc
    epoch_result["Val Acc"] = val_acc
    trianing_result.append(epoch_result)

    if val_loss < best_valid_loss:
      best_valid_loss = val_loss
      best_model_params = copy.deepcopy(model.state_dict())

  return trianing_result, best_model_params


def evaluate_model(model, val_loader, criterion, device='cuda'):
  model.eval()
  val_loss = 0.0
  correct = 0
  total = 0
  with torch.no_grad():
      for images, labels in val_loader:
          images, labels = images.to(device), labels.to(device)
          outputs = model(images)
          loss = criterion(outputs, labels)
          val_loss += loss.item() * images.size(0)
          _, predicted = torch.max(outputs, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()

  val_loss = val_loss / len(val_loader.dataset)
  val_acc = correct / total

  print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
  return val_loss, val_acc

def reset_seeds(seed=42):
    """
    Reset seeds for reproducibility.

    Args:
    - seed: Integer seed value (default is 42).
    """
    # Python's built-in random module
    random.seed(seed)

    # NumPy
    np.random.seed(seed)

    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.

    # Ensure reproducibility when using CuDNN
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def save_model(model_save_path, model_type, trianing_result, best_model_params, classes):
  current_time = datetime.datetime.now()
  time_string = current_time.strftime("%Y-%m-%d_%H-%M")

  model_filename = "_".join([model_type, time_string]) + ".pth"

  model_file_path = os.path.join(model_save_path, model_filename)
  torch.save(best_model_params, model_file_path)

  
  json_content = {}
  json_content["classes"] = classes
  json_content["trianing_process"] = trianing_result

  json_filename = "_".join([model_type, time_string]) + ".json"
  json_file_path = os.path.join(model_save_path, json_filename)
  with open(json_file_path, 'w') as json_file:
    json.dump(json_content, json_file)


if __name__=="__main__":

  parser = argparse.ArgumentParser(description="Unzip a file.")

  # Add arguments
  parser.add_argument("--batch_size", type=int, default=64)
  parser.add_argument("--epochs", type=int, default=50)
  parser.add_argument("--lr", type=float, default=0.001)
  parser.add_argument("--seed", type=int, default=42)
  parser.add_argument("--transform", type=str, choices=['default_transform', 'canny_transform'], default='default_transform')

  parser.add_argument("--model", type=str, choices=['resnet50', 'BiT', 'Ensemble'], default='resnet50', help="not implemented")
  parser.add_argument("--train_data_path", type=str, help="Path to load data")
  parser.add_argument("--model_save_path", type=str, help="Path to save the model's weight")
  

  # Parse arguments
  args = parser.parse_args()

  batch_size = args.batch_size
  epochs = args.epochs
  model_type = args.model
  model_save_path = args.model_save_path
  num_classes = 121
  train_data_path = args.train_data_path
  lr=args.lr
  seed = args.seed

  transform = default_transform
  if(args.transform == 'canny_transform'):
    transform = canny_transform
     

  reset_seeds(seed)


  """## data loading ##"""
  dataloader = get_dataloader(train_data_path, batch_size, True, transform)
  validation_size = int(0.2 * len(dataloader.dataset))
  train_dataset, val_dataset = random_split(dataloader.dataset, [len(dataloader.dataset) - validation_size, validation_size])

  # Create new DataLoaders for the training and validation datasets
  train_loader = DataLoader(train_dataset, batch_size=dataloader.batch_size, shuffle=True, num_workers=dataloader.num_workers)
  val_loader = DataLoader(val_dataset, batch_size=dataloader.batch_size, shuffle=False, num_workers=dataloader.num_workers)


  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model = ResNetClassifier(num_classes=num_classes)
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

  """##training##"""
  trianing_result, best_model_params = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=epochs, device='cuda')
  save_model(model_save_path, model_type, trianing_result, best_model_params, dataloader.dataset.classes)
